{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1],\n",
    "])\n",
    "\n",
    "y = np.atleast_2d([0, 1, 1, 0]).T\n",
    "\n",
    "print('X.shape:', X.shape)\n",
    "print('y.shape:', y.shape)\n",
    "\n",
    "# [2, 2, 1] will also work for the XOR problem presented\n",
    "LAYERS = [2, 2, 2, 1]\n",
    "ETA = .1\n",
    "THETA = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# activation and derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "def sigmoid_prime(x):\n",
    "    return sigmoid(x) * (1-sigmoid(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# forward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def initialize_parameters():\n",
    "    for idx in range(1, len(LAYERS)):\n",
    "        THETA.append(np.random.rand(LAYERS[idx], LAYERS[idx-1]+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def forward_propagation(X,initialize=True):\n",
    "    if initialize:\n",
    "        initialize_parameters()\n",
    "    # adding bias column to the input X\n",
    "    A = [np.hstack((np.ones((X.shape[0],1)), X))]\n",
    "    Z = []\n",
    "    activate = False\n",
    "    for idx, theta in enumerate(THETA):\n",
    "        Z.append(np.matmul(A[-1], theta.T))\n",
    "        # adding bias column to the output of previous layer\n",
    "        A.append(np.hstack((np.ones((Z[-1].shape[0],1)), sigmoid(Z[-1]))))\n",
    "    # bias is not needed in the final output\n",
    "    A[-1] = A[-1][:, 1:]\n",
    "    y_hat = A[-1]\n",
    "    return A, Z, y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THETA = []\n",
    "A, Z, y_hat = forward_propagation(X)\n",
    "print('THETA=> \\t', [_.shape for _ in THETA])\n",
    "print(\"A=> \\t\\t\", [_.shape for _ in A])\n",
    "print(\"Z=> \\t\\t\", [_.shape for _ in Z])\n",
    "print(\"y_hat=> \\t\", y_hat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backward propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def back_propagation(X, y, initialize=True, debug=False, verbose=False):\n",
    "    # run a forward pass\n",
    "    A, Z, y_hat = forward_propagation(X, initialize)\n",
    "    # calculate delta at final output\n",
    "    del_ = [(y_hat - y) * sigmoid_prime(Z[2])]\n",
    "    if verbose:\n",
    "        print(np.mean([_ * _ for _ in del_]))\n",
    "    # flag to signify whether a layer has bias column of not\n",
    "    bias_free = True\n",
    "    # running in reverse because delta is propagated backwards\n",
    "    for idx in reversed(range(1, len(THETA))):\n",
    "        if bias_free:\n",
    "            # true only for the final layer where there is no bias\n",
    "            temp = np.matmul(del_[0], THETA[idx]) * np.hstack((np.ones((Z[idx-1].shape[0], 1)), sigmoid_prime(Z[idx-1])))\n",
    "            bias_free=False\n",
    "        else:\n",
    "            # true for all the layers except the input and output layer\n",
    "            temp = np.matmul(del_[0][:,1:], THETA[idx]) * np.hstack((np.ones((Z[idx-1].shape[0], 1)), sigmoid_prime(Z[idx-1])))\n",
    "        del_ = [temp] + del_\n",
    "    del_theta = []\n",
    "    bias_free = True\n",
    "    # calculation for the delta in the parameters\n",
    "    for idx in reversed(range(len(del_))):\n",
    "        if bias_free:\n",
    "            # true only for the final layer where there is no bias\n",
    "            del_theta = [-ETA * np.matmul(del_[idx].T, A[idx])] + del_theta\n",
    "            bias_free = False\n",
    "        else:\n",
    "            # true for all the layers except the input and output layer\n",
    "            del_theta = [-ETA * np.matmul(del_[idx][:, 1:].T, A[idx])] + del_theta\n",
    "    # update parameters\n",
    "    for idx in range(len(THETA)):\n",
    "        # asserting that the matrix sizes are same\n",
    "        assert THETA[idx].shape == del_theta[idx].shape\n",
    "        THETA[idx] = THETA[idx] + del_theta[idx]\n",
    "    if debug:\n",
    "        return (A, Z, y_hat, del_, del_theta)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "THETA=[]\n",
    "\n",
    "A, Z, y_hat, del_, del_theta = back_propagation(X, y, True, True, verbose=True)\n",
    "print('THETA=> \\t', [_.shape for _ in THETA])\n",
    "print(\"A=> \\t\\t\", [_.shape for _ in A])\n",
    "print(\"Z=> \\t\\t\", [_.shape for _ in Z])\n",
    "print(\"y_hat=> \\t\", [y_hat.shape])\n",
    "print(\"del_=> \\t\\t\", [_.shape for _ in del_])\n",
    "print('del_theta=> \\t', [_.shape for _ in del_theta])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "initialize=True\n",
    "verbose=True\n",
    "THETA=[]\n",
    "for i in range(20000):\n",
    "    if i % 1000 == 0:\n",
    "        verbose=True\n",
    "    back_propagation(X, y, initialize, debug=False, verbose=verbose)\n",
    "    verbose=False\n",
    "    initialize=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "A, Z, y_hat = forward_propagation(X, initialize=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
